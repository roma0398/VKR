{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm.notebook import tqdm\n",
    "from implicit.bpr import BayesianPersonalizedRanking\n",
    "from implicit.evaluation import train_test_split, precision_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/25m'\n",
    "\n",
    "# we are working with movie data, but we'll name\n",
    "# the movie as item to make it more generic to\n",
    "# all use-cases\n",
    "user_col = 'userId'\n",
    "item_col = 'movieId'\n",
    "value_col = 'rating'\n",
    "time_col = 'timestamp'\n",
    "\n",
    "rating_path = os.path.join(data_dir, 'ratings.csv')\n",
    "df_raw = pd.read_csv(rating_path)\n",
    "print('dimension: ', df_raw.shape)\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_col = 'title'\n",
    "genre_col = 'genres'\n",
    "\n",
    "item_info_path = os.path.join(data_dir, 'movies.csv')\n",
    "df_item = pd.read_csv(item_info_path)\n",
    "df_item = df_item[df_item[genre_col] != '(no genres listed)']\n",
    "print('dimension: ', df_item.shape)\n",
    "df_item.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Item:\n",
    "    \"\"\"\n",
    "    Data holder for our item.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    id : int\n",
    "  \n",
    "    title : str\n",
    "\n",
    "    genre : dict[str, float]\n",
    "        The item/movie's genre distribution, where the key\n",
    "        represents the genre and value corresponds to the\n",
    "        ratio of that genre.\n",
    "\n",
    "    score : float\n",
    "        Score for the item, potentially generated by some\n",
    "        recommendation algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self, _id, title, genres, score=None):\n",
    "        self.id = _id\n",
    "        self.title = title\n",
    "        self.score = score\n",
    "        self.genres = genres\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.title\n",
    "\n",
    "\n",
    "def create_item_mapping(df_item, item_col, title_col, genre_col):\n",
    "    \"\"\"Create a dictionary of item id to Item lookup.\"\"\"\n",
    "    item_mapping = {}\n",
    "    for row in df_item.itertuples():\n",
    "        item_id = getattr(row, item_col)\n",
    "        item_title = getattr(row, title_col)\n",
    "        item_genre = getattr(row, genre_col)\n",
    "\n",
    "        splitted = item_genre.split('|')\n",
    "        genre_ratio = 1. / len(splitted)\n",
    "        item_genre = {genre: genre_ratio for genre in splitted}\n",
    "\n",
    "        item = Item(item_id, item_title, item_genre)\n",
    "        item_mapping[item_id] = item\n",
    "\n",
    "    return item_mapping\n",
    "    \n",
    "\n",
    "item_mapping = create_item_mapping(df_item, item_col, title_col, genre_col)\n",
    "item_mapping[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to implicit feedback data and filter out\n",
    "# movies that doesn't have any genre\n",
    "df_rating = df_raw[df_raw[value_col] >= 4.0].copy()\n",
    "df_rating = df_rating.merge(df_item, on=item_col)\n",
    "\n",
    "for col in (user_col, item_col):\n",
    "    df_rating[col] = df_rating[col].astype('category')\n",
    "\n",
    "# the original id are converted to indices to create\n",
    "# the sparse matrix, so we keep track of the mappings here\n",
    "# e.g. a userId 1 will correspond to index 0 in our sparse matrix\n",
    "index2user = df_rating[user_col].cat.categories\n",
    "index2item = df_rating[item_col].cat.categories\n",
    "\n",
    "print('dimension: ', df_rating.shape)\n",
    "df_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_item_csr_matrix(data, user_col, item_col, value_col):\n",
    "    rows = data[user_col].cat.codes\n",
    "    cols = data[item_col].cat.codes\n",
    "    values = data[value_col].astype(np.float32)\n",
    "    return csr_matrix((values, (rows, cols)))\n",
    "\n",
    "\n",
    "user_item = create_user_item_csr_matrix(df_rating, user_col, item_col, value_col)\n",
    "user_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "user_item_train, user_item_test = train_test_split(user_item, train_percentage=0.8)\n",
    "user_item_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model expects item-user sparse matrix,\n",
    "# i.e. the rows represents item and the column\n",
    "# represents users\n",
    "np.random.seed(1234)\n",
    "bpr = BayesianPersonalizedRanking(iterations=70)\n",
    "bpr.fit(user_item_train.T.tocsr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_at_k(bpr, user_item_train, user_item_test, K=10)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look a the first user\n",
    "user_id = 1\n",
    "\n",
    "# find the index that the user interacted with,\n",
    "# we can then map this to a list of Item, note that we need to first\n",
    "# map the recommended index to the actual itemId/movieId first\n",
    "interacted_ids = user_item_train[user_id].nonzero()[1]\n",
    "interacted_items = [item_mapping[index2item[index]] for index in interacted_ids]\n",
    "interacted_items[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it returns the recommended index and their corresponding score\n",
    "topn = 30\n",
    "reco = bpr.recommend(user_id, user_item_train, N=topn)\n",
    "reco[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the index to Item\n",
    "reco_items = [item_mapping[index2item[index]] for index, _ in reco]\n",
    "reco_items[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_genre_distr(items):\n",
    "    \"\"\"Compute the genre distribution for a given list of Items.\"\"\"\n",
    "    distr = {}\n",
    "    for item in items:\n",
    "        for genre, score in item.genres.items():\n",
    "            genre_score = distr.get(genre, 0.)\n",
    "            distr[genre] = genre_score + score\n",
    "\n",
    "    # we normalize the summed up probability so it sums up to 1\n",
    "    # and round it to three decimal places, adding more precision\n",
    "    # doesn't add much value and clutters the output\n",
    "    for item, genre_score in distr.items():\n",
    "        normed_genre_score = round(genre_score / len(items), 3)\n",
    "        distr[item] = normed_genre_score\n",
    "\n",
    "    return distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can check that the probability does in fact add up to 1\n",
    "# np.array(list(interacted_distr.values())).sum()\n",
    "interacted_distr = compute_genre_distr(interacted_items)\n",
    "interacted_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 0\n",
    "summ = 0\n",
    "for user_id in range(2400):\n",
    "    \n",
    "# find the index that the user interacted with,\n",
    "# we can then map this to a list of Item, note that we need to first\n",
    "# map the recommended index to the actual itemId/movieId first\n",
    "    interacted_ids = user_item_train[user_id].nonzero()[1]\n",
    "    interacted_items = [item_mapping[index2item[index]] for index in interacted_ids]\n",
    "    interacted_items[:10]\n",
    "    interacted_distr = compute_genre_distr(interacted_items)\n",
    "    import operator\n",
    "    n = 0\n",
    "    p = interacted_distr.copy()\n",
    "    r = {x:0 for x in interacted_distr.keys()}\n",
    "    while n < 30:\n",
    "        m = max(p.items(), key=operator.itemgetter(1))[0]\n",
    "        r[m] += 1\n",
    "        n += 1\n",
    "        p[m] = p[m] / (2*r[m]+1)\n",
    "        \n",
    "    for i, j in r.items():\n",
    "        r[i] = j/30\n",
    "        \n",
    "    summ += compute_kl_divergence(interacted_distr, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ / 2400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_distr = compute_genre_distr(reco_items)\n",
    "reco_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change default style figure and font size\n",
    "plt.rcParams['figure.figsize'] = 10, 8\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "\n",
    "def distr_comparison_plot(interacted_distr, reco_distr, width=0.3):\n",
    "    \n",
    "    # the value will automatically be converted to a column with the\n",
    "    # column name of '0'\n",
    "    interacted = pd.DataFrame.from_dict(interacted_distr, orient='index')\n",
    "    reco = pd.DataFrame.from_dict(reco_distr, orient='index')\n",
    "    df = interacted.join(reco, how='outer', lsuffix='_interacted')\n",
    "\n",
    "    n = df.shape[0]\n",
    "    index = np.arange(n)\n",
    "    plt.barh(index, df['0_interacted'], height=width, label='interacted distr')\n",
    "    plt.barh(index + width, df['0'], height=width, label='reco distr')\n",
    "    plt.yticks(index, df.index)\n",
    "    plt.legend(bbox_to_anchor=(1, 0.5))\n",
    "    plt.title('Genre Distribution between User Historical Interaction v.s. Recommendation')\n",
    "    plt.ylabel('Genre')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "distr_comparison_plot(interacted_distr, reco_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interacted_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kl_divergence(interacted_distr, reco_distr, alpha=0.02):\n",
    "    \"\"\"\n",
    "    KL (p || q), the lower the better.\n",
    "\n",
    "    alpha is not really a tuning parameter, it's just there to make the\n",
    "    computation more numerically stable.\n",
    "    \"\"\"\n",
    "    kl_div = 0.\n",
    "    for genre, score in interacted_distr.items():\n",
    "        reco_score = reco_distr.get(genre, 0.)\n",
    "        reco_score = (1 - alpha) * reco_score + alpha * score\n",
    "        if reco_score==0:\n",
    "            kl_div += score\n",
    "        else:\n",
    "            kl_div += score * np.log2(score / reco_score)\n",
    "\n",
    "    return kl_div\n",
    "\n",
    "\n",
    "compute_kl_divergence(interacted_distr, reco_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_item_candidates(model, user_item, user_id, index2item, item_mapping,\n",
    "                             filter_already_liked_items=True):\n",
    "    \"\"\"\n",
    "    For a given user, generate the list of items that we can recommend, during this\n",
    "    step, we will also attach the recommender's score to each item.\n",
    "    \"\"\"\n",
    "    n_items = user_item.shape[1]\n",
    "    \n",
    "    # this is how implicit's matrix factorization generates\n",
    "    # the scores for each item for a given user, modify this\n",
    "    # part of the logic if we were to use a completely different\n",
    "    # algorithm to generate the ranked items\n",
    "    user_factor = model.user_factors[user_id]\n",
    "    scores = model.item_factors.dot(user_factor)\n",
    "\n",
    "    liked = set()\n",
    "    if filter_already_liked_items:\n",
    "        liked = set(user_item[user_id].indices)\n",
    "\n",
    "    item_ids = set(np.arange(n_items))\n",
    "    item_ids -= liked\n",
    "\n",
    "    items = []\n",
    "    for item_id in item_ids:\n",
    "        item = item_mapping[index2item[item_id]]\n",
    "        item.score = scores[item_id]\n",
    "        items.append(item)\n",
    "\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = generate_item_candidates(bpr, user_item_train, user_id, index2item, item_mapping)\n",
    "print('number of item candidates:', len(items))\n",
    "items[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_utility(reco_items, interacted_distr, lmbda=0.5):\n",
    "    \"\"\"\n",
    "    Our objective function for computing the utility score for\n",
    "    the list of recommended items.\n",
    "\n",
    "    lmbda : float, 0.0 ~ 1.0, default 0.5\n",
    "        Lambda term controls the score and calibration tradeoff,\n",
    "        the higher the lambda the higher the resulting recommendation\n",
    "        will be calibrated. Lambda is keyword in Python, so it's\n",
    "        lmbda instead ^^\n",
    "    \"\"\"\n",
    "    reco_distr = compute_genre_distr(reco_items)\n",
    "    kl_div = compute_kl_divergence(interacted_distr, reco_distr)\n",
    "\n",
    "    total_score = 0.0\n",
    "    for item in reco_items:\n",
    "        total_score += item.score\n",
    "    \n",
    "    # kl divergence is the lower the better, while score is\n",
    "    # the higher the better so remember to negate it in the calculation\n",
    "    utility = (1 - lmbda) * total_score - lmbda * kl_div\n",
    "    return utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calib_recommend(items, interacted_distr, topn, lmbda=0.5):\n",
    "    \"\"\"\n",
    "    start with an empty recommendation list,\n",
    "    loop over the topn cardinality, during each iteration\n",
    "    update the list with the item that maximizes the utility function.\n",
    "    \"\"\"\n",
    "    calib_reco = []\n",
    "    for _ in range(topn):\n",
    "        max_utility = -np.inf\n",
    "        for item in items:\n",
    "            if item in calib_reco:\n",
    "                continue\n",
    "\n",
    "            utility = compute_utility(calib_reco + [item], interacted_distr, lmbda)\n",
    "            if utility > max_utility:\n",
    "                max_utility = utility\n",
    "                best_item = item\n",
    "\n",
    "        calib_reco.append(best_item)\n",
    "        \n",
    "    return calib_reco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "calib_reco_items = calib_recommend(items, interacted_distr, topn, lmbda=0.99)\n",
    "elapsed = time.time() - start\n",
    "print('elapsed: ', elapsed)\n",
    "calib_reco_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_reco_distr = compute_genre_distr(calib_reco_items)\n",
    "calib_reco_kl_div = compute_kl_divergence(interacted_distr, calib_reco_distr)\n",
    "reco_kl_div = compute_kl_divergence(interacted_distr, reco_distr)\n",
    "print('\\noriginal reco kl-divergence score:', reco_kl_div)\n",
    "print('calibrated reco kl-divergence score:', calib_reco_kl_div)\n",
    "\n",
    "distr_comparison_plot(interacted_distr, calib_reco_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(user_item, user_id, reco_items, index2item):\n",
    "    indptr = user_item.indptr\n",
    "    indices = user_item.indices\n",
    "\n",
    "    reco_ids = {item.id for item in reco_items}\n",
    "    likes = {index2item[indices[i]] for i in range(indptr[user_id], indptr[user_id + 1])}\n",
    "\n",
    "    relevant = len(reco_ids & likes)\n",
    "    total = min(len(reco_items), len(likes))\n",
    "    return relevant / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_precision = precision(user_item_test, user_id, reco_items, index2item)\n",
    "calib_reco_precision = precision(user_item_test, user_id, calib_reco_items, index2item)\n",
    "print('original reco precision score:', reco_precision)\n",
    "print('calibrated reco precision score:', calib_reco_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "calib_reco_items = calib_recommend(items, interacted_distr, topn, lmbda=0.5)\n",
    "elapsed = time.time() - start\n",
    "print('elapsed: ', elapsed)\n",
    "calib_reco_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_reco_distr = compute_genre_distr(calib_reco_items)\n",
    "calib_reco_kl_div = compute_kl_divergence(interacted_distr, calib_reco_distr)\n",
    "calib_reco_precision = precision(user_item_test, user_id, calib_reco_items, index2item)\n",
    "print('calibrated reco kl-divergence score:', calib_reco_kl_div)\n",
    "print('calibrated reco precision score:', calib_reco_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_reco_distr = compute_genre_distr(calib_reco_items)\n",
    "distr_comparison_plot(interacted_distr, calib_reco_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topn = 30\n",
    "user_id = 100\n",
    "lmbda = 0.9\n",
    "\n",
    "prec_cal = 0\n",
    "kl_cal = 0\n",
    "kl_rec = 0\n",
    "prec_rec = 0\n",
    "n = 0\n",
    "\n",
    "for user_id in tqdm(range(1, 11)):\n",
    "    try:\n",
    "        reco = bpr.recommend(user_id, user_item_train, N=topn)\n",
    "        reco_items = [item_mapping[index2item[index]] for index, _ in reco]\n",
    "        reco_distr = compute_genre_distr(reco_items)\n",
    "\n",
    "        interacted_ids = user_item_train[user_id].nonzero()[1]\n",
    "        interacted_items = [item_mapping[index2item[index]] for index in interacted_ids]\n",
    "        interacted_distr = compute_genre_distr(interacted_items)\n",
    "\n",
    "        items = generate_item_candidates(bpr, user_item_train, user_id, index2item, item_mapping)\n",
    "        calib_reco_items = calib_recommend(items, interacted_distr, topn, lmbda)\n",
    "        calib_reco_distr = compute_genre_distr(calib_reco_items)\n",
    "\n",
    "        calib_reco_kl_div = compute_kl_divergence(interacted_distr, calib_reco_distr)\n",
    "        calib_reco_precision = precision(user_item_test, user_id, calib_reco_items, index2item)\n",
    "        #print('calibrated reco kl-divergence score:', calib_reco_kl_div)\n",
    "        #print('calibrated reco precision score:', calib_reco_precision)\n",
    "        #distr_comparison_plot(interacted_distr, calib_reco_distr)\n",
    "\n",
    "        reco_kl_div = compute_kl_divergence(interacted_distr, reco_distr)\n",
    "        reco_precision = precision(user_item_test, user_id, reco_items, index2item)\n",
    "        #print('original reco kl-divergence score:', reco_kl_div)\n",
    "        #print('original reco precision score:', reco_precision)\n",
    "        #distr_comparison_plot(interacted_distr, reco_distr)\n",
    "\n",
    "        prec_rec += reco_precision\n",
    "        prec_cal += calib_reco_precision\n",
    "        kl_rec += reco_kl_div\n",
    "        kl_cal += calib_reco_kl_div\n",
    "        n += 1\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print('calibrated reco kl-divergence score:', kl_cal/n)\n",
    "print('calibrated reco precision score:', prec_cal/n)\n",
    "print('original reco kl-divergence score:', kl_rec/n)\n",
    "print('original reco precision score:', prec_rec/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
